{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务1：PyTorch搭建LeNet模型进行MNIST分类\n",
    "\n",
    "**任务要求：利用PyTorch框架搭建一个LeNet模型，并针对MNIST数据集进行训练和测试。**  \n",
    "  \n",
    "同样的，__你需要在TODO标注部分填写你的代码。__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: False\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "print('Use GPU:', use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据集加载\n",
    "\n",
    "利用torchvision的datasets加载MNSIT数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像处理方式\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5],[0.5])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cd374346244f85bff7d82051858d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b14e1360264c48a5f3621f20fbca9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df27dc0c930641758352f5ab4d4dac2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071e776ce39444388e7a8d6fc01d4037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5], std=[0.5])\n",
      "           )\n",
      "train image num: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larry/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# 加载训练数据集\n",
    "batch_size = 64\n",
    "train_dataset = datasets.MNIST(root='./data/', train=True, transform=transform, download=True)\n",
    "print(train_dataset)\n",
    "\n",
    "# 创建训练dataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_num = len(train_dataset)\n",
    "print('train image num:', train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5], std=[0.5])\n",
      "           )\n",
      "test image num: 10000\n"
     ]
    }
   ],
   "source": [
    "# 加载测试数据集\n",
    "test_dataset = datasets.MNIST(root='./data/', train=False, transform=transform, download=True)\n",
    "print(test_dataset)\n",
    "\n",
    "# 创建测试dataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_num = len(test_dataset)\n",
    "print('test image num:', test_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LeNet模型构建\n",
    "\n",
    "利用PyTorch构建LeNet-5模型  \n",
    "\n",
    "友情链接(将会使用到的pytorch语句)：[nn.Conv2d()](https://blog.csdn.net/qq_38863413/article/details/104108808) / [nn.MaxPool2d()](https://blog.csdn.net/qq_36387683/article/details/107638184) / [nn.Linear()](https://blog.csdn.net/qq_42079689/article/details/102873766) / [F.relu()](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html?highlight=f%20relu#torch.nn.functional.relu)  \n",
    "\n",
    "[卷积前后图像大小变化公式(定义卷积操作时需要计算)](https://blog.csdn.net/qq_32256033/article/details/103345337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![jupyter](./data/lenet_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# TODO:                                                               #\n",
    "# 使用pytorch搭建LeNet模型                                            #\n",
    "#######################################################################\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        ''' \n",
    "        请在此处定义LeNet模型将会用到的模块\n",
    "        \n",
    "        提示：\n",
    "        包括卷积层，采样层，全连接层，激活函数等\n",
    "        你将会用到nn.Conv2d()/nn.MaxPool2d()/nn.Linear()/F.relu()等pytorch接口函数，\n",
    "        \n",
    "        例如，你可以定义某个卷积操作为\n",
    "        self.conv = nn.Conv2d(......)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        LeNet网络结构：\n",
    "            input -> 卷积层1 -> 激活 -> 池化 ->卷积层2 -> 激活 -> 池化 -> fatten -> fc1 -> fc2 > fc3\n",
    "        '''\n",
    "        # 通道数由1变为6 所以chan_in = 1 chan_out = 6\n",
    "        # 网络结构里的input是32*32 但是数据集的图片 是28*28 所以周围补上2个0 使得28*28 变为 32*32 所以padding=2\n",
    "        # 尺寸从32变为28 计算可得 卷积核大小为5\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5,padding=2)\n",
    "        # 激活函数选择relu\n",
    "        self.activate = nn.ReLU()\n",
    "        # 由上面的网络结构图可知 池化层的核大小为（2，2）\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2)\n",
    "        # 通道数由6变为16 所以chan_in = 6 chan_out = 16\n",
    "        # 尺寸从14变为28 计算可得 卷积核大小为10\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5)\n",
    "        # 将feature map平铺成向量\n",
    "        self.fatten = nn.Flatten()\n",
    "        # 全连接层 从图中可得 输入神经元16*5*5个 输出神经元120个\n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        # 全连接层 从图中可得 输入神经元120个 输出神经元84个\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        # 全连接层 从图中可得 输入神经元84个 输出神经元10个 之所以最后是10  是因为数字0-9是10个类别\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ''' 请在此处编写LeNet模型处理图像的过程 '''\n",
    "        \n",
    "        # 提示\n",
    "        # 与上图模型结构图不同的是，输入的图像形状为28×28\n",
    "        # 即x: (64, 1, 28, 28)\n",
    "\n",
    "        # C1卷积阶段\n",
    "        # 你需要调用定义的卷积操作，以得到shape为(64, 6, 28, 28)的特征\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # 激活函数，你需要用到relu函数\n",
    "        x =self.activate(x)\n",
    "\n",
    "        # S2池化阶段\n",
    "        # 你需要调用定义的池化操作，以得到shape为(64, 6, 14, 14)的特征\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        # C3卷积阶段\n",
    "        # 你需要调用定义的卷积操作，以得到shape为(64, 16, 10, 10)的特征\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # 激活函数，你需要用到relu函数\n",
    "        x = self.activate(x)\n",
    "\n",
    "        # S4池化阶段\n",
    "        # 你需要调用定义的池化操作，以得到shape为(64, 16, 5, 5)的特征\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        # 此阶段你需要将特征平铺，以得到shape为(64, 400)的特征\n",
    "        x = self.fatten(x)\n",
    "\n",
    "        # C5全连接阶段\n",
    "        # 你需要调用定义的全连接操作，以得到shape为(64, 120)的特征\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        # 激活函数，你需要用到relu函数\n",
    "        x =self.activate(x)\n",
    "\n",
    "        # F6全连接阶段\n",
    "        # 你需要调用定义的全连接操作，以得到shape为(64, 84)的特征\n",
    "        x =self.fc2(x)\n",
    "\n",
    "        # 激活函数，你需要用到relu函数\n",
    "        x = self.activate(x)\n",
    "\n",
    "        # OUTPUT\n",
    "        # 你需要调用定义的全连接操作，以得到shape为(64, 10)的特征\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (activate): ReLU()\n",
      "  (pooling): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 实例化\n",
    "cnn = LeNet()\n",
    "if use_gpu:\n",
    "    cnn = cnn.cuda()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 优化器和损失函数\n",
    "\n",
    "定义优化器(SGD/Adam)和交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# 优化器\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=learning_rate, betas=(0.9, 0.99))\n",
    "print(optimizer)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型训练和测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练CNN模型\n",
    "def train(epoch):\n",
    "    cnn.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        if use_gpu:  # 使用GPU\n",
    "            data, label = Variable(data).cuda(), Variable(label).cuda()\n",
    "        \n",
    "        # 模型预测结果\n",
    "        predict = cnn(data)\n",
    "        \n",
    "        # 计算损失函数\n",
    "        loss = criterion(predict, label)     \n",
    "        pred = predict.data.argmax(1)\n",
    "        correct += (pred==label).sum().item()\n",
    "        \n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 更新网络参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 输出训练阶段loss信息\n",
    "        train_loss += loss.item()\n",
    "        train_loss_list.append(loss.item())\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), train_num,\n",
    "                100 * batch_idx * len(data) / train_num, loss.item()))\n",
    "\n",
    "    train_loss /= train_num\n",
    "    accuracy = correct / train_num\n",
    "    train_acc_list.append(accuracy)\n",
    "    # 输出训练阶段loss信息\n",
    "    print('Train Epoch: {}\\tAverage loss: {:.4f}\\tAccuracy: {}/{} ({:.2f}%)'.format(\n",
    "        epoch, train_loss, correct, train_num, 100.0 * accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试CNN模型\n",
    "def test():\n",
    "    cnn.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, label in test_loader:\n",
    "        if use_gpu:  # 使用GPU\n",
    "            data, label = Variable(data).cuda(), Variable(label).cuda()\n",
    "        \n",
    "        # 模型预测结果\n",
    "        predict = cnn(data)\n",
    "        # 计算batch损失和\n",
    "        loss = criterion(predict, label)\n",
    "        # 预测label\n",
    "        pred = predict.data.argmax(1)\n",
    "        # 预测正确数\n",
    "        correct += (pred==label).sum().item()\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        test_loss_list.append(loss.item())\n",
    "\n",
    "    test_loss /= test_num\n",
    "    accuracy = correct / test_num\n",
    "    test_acc_list.append(accuracy)\n",
    "    # 输出测试阶段loss信息\n",
    "    print('Test Epoch: {}\\tAverage loss: {:.4f}\\tAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        epoch, test_loss, correct, test_num, 100.0 * accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [00000/60000 (0%)]\tLoss: 2.297857\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.179144\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.140803\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.134280\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.028391\n",
      "Train Epoch: 1\tAverage loss: 0.0042\tAccuracy: 54890/60000 (91.48%)\n",
      "Test Epoch: 1\tAverage loss: 0.0011\tAccuracy: 9774/10000 (97.74%)\n",
      "\n",
      "Train Epoch: 2 [00000/60000 (0%)]\tLoss: 0.085193\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.206651\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.049295\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.097296\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.113343\n",
      "Train Epoch: 2\tAverage loss: 0.0010\tAccuracy: 58784/60000 (97.97%)\n",
      "Test Epoch: 2\tAverage loss: 0.0007\tAccuracy: 9862/10000 (98.62%)\n",
      "\n",
      "Train Epoch: 3 [00000/60000 (0%)]\tLoss: 0.016773\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.048347\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.071061\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.050251\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.007647\n",
      "Train Epoch: 3\tAverage loss: 0.0007\tAccuracy: 59137/60000 (98.56%)\n",
      "Test Epoch: 3\tAverage loss: 0.0007\tAccuracy: 9855/10000 (98.55%)\n",
      "\n",
      "Train Epoch: 4 [00000/60000 (0%)]\tLoss: 0.027136\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.008764\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.052665\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.018086\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.046287\n",
      "Train Epoch: 4\tAverage loss: 0.0005\tAccuracy: 59328/60000 (98.88%)\n",
      "Test Epoch: 4\tAverage loss: 0.0004\tAccuracy: 9916/10000 (99.16%)\n",
      "\n",
      "Train Epoch: 5 [00000/60000 (0%)]\tLoss: 0.026890\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.012067\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.011427\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.035918\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.007021\n",
      "Train Epoch: 5\tAverage loss: 0.0004\tAccuracy: 59466/60000 (99.11%)\n",
      "Test Epoch: 5\tAverage loss: 0.0005\tAccuracy: 9908/10000 (99.08%)\n",
      "\n",
      "Train Epoch: 6 [00000/60000 (0%)]\tLoss: 0.008415\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.056673\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.012078\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.006098\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.028827\n",
      "Train Epoch: 6\tAverage loss: 0.0004\tAccuracy: 59575/60000 (99.29%)\n",
      "Test Epoch: 6\tAverage loss: 0.0005\tAccuracy: 9907/10000 (99.07%)\n",
      "\n",
      "Train Epoch: 7 [00000/60000 (0%)]\tLoss: 0.008687\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.033337\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.052175\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.158244\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.027975\n",
      "Train Epoch: 7\tAverage loss: 0.0003\tAccuracy: 59622/60000 (99.37%)\n",
      "Test Epoch: 7\tAverage loss: 0.0005\tAccuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Train Epoch: 8 [00000/60000 (0%)]\tLoss: 0.000931\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.002691\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.006383\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.021725\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003365\n",
      "Train Epoch: 8\tAverage loss: 0.0003\tAccuracy: 59678/60000 (99.46%)\n",
      "Test Epoch: 8\tAverage loss: 0.0006\tAccuracy: 9888/10000 (98.88%)\n",
      "\n",
      "Train Epoch: 9 [00000/60000 (0%)]\tLoss: 0.004666\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.021762\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.088083\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "epoch_num = 10\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(1, epoch_num+1):\n",
    "    # 每轮训练完测试\n",
    "    train(epoch)\n",
    "    test()\n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"时间开销：\"+str(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 损失函数和正确率曲线\n",
    "\n",
    "训练损失函数图 & 测试损失函数图 & 训练/测试正确率图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_loss_list)\n",
    "plt.title('train loss', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(test_loss_list)\n",
    "plt.title('test loss', fontsize=18)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(train_acc_list, 'o-')\n",
    "plt.plot(test_acc_list, 'o-')\n",
    "plt.title('Accuracy', fontsize=18)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(['Train', 'Test'], fontsize=16)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务2：torchvision预训练模型测试真实图像分类\n",
    "\n",
    "**任务要求：利用torchvision中的预训练CNN模型来对真实的图像进行分类，预测每张图片的top5类别。**  \n",
    "**数据: real_image, class_index.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 类别索引\n",
    "\n",
    "构建类别索引词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/class_index.json')\n",
    "class_index = json.load(f)\n",
    "print('class num:', len(class_index))\n",
    "class_dict = {int(k): v[1] for k, v in class_index.items()}\n",
    "# print(class_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 预训练模型\n",
    "\n",
    "加载预训练CNN模型。[参考链接](https://pytorch.org/vision/stable/models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# TODO:                                                               #\n",
    "# 加载预训练CNN模型                                                   #\n",
    "#######################################################################\n",
    "\n",
    "# 请在下方编写加载torchvision预训练模型代码\n",
    "# 你可以对比多个模型的结果，例如alexnet/googlenet/resnet\n",
    "# 注意pretrained参数设为True\n",
    "\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "\n",
    "# 将模型设为测试模型，你将会用到.eval()方法\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 图像预处理\n",
    "\n",
    "图像缩放、裁剪、转Tensor、归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像预处理转换代码\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 测试数据集加载\n",
    "\n",
    "构建测试数据集，迭代返回预处理后的Tensor格式图像和原始图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset():\n",
    "    def __init__(self, root, transforms=None):\n",
    "        imgs = os.listdir(root)\n",
    "        self.imgs = [os.path.join(root, img) for img in imgs]\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.imgs[index]\n",
    "        img_pil = Image.open(img_path)\n",
    "        label = None\n",
    "        img_np = np.asarray(img_pil)\n",
    "        data = self.transforms(img_pil)\n",
    "        return data, img_np\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = './data/real_image/'\n",
    "test_dataset = TestDataset(test_dir, image_transforms)\n",
    "print('test image num:', test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 模型预测图像类别\n",
    "\n",
    "在测试模式下，对于每张图片显示原始图像，并输出模型预测的top5类别及top1类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 预测图像类别\n",
    "for i, (data, img_np) in enumerate(test_dataset):\n",
    "    # 显示原始图像\n",
    "    plt.imshow(img_np)\n",
    "    plt.title('image-%d' % (i+1), fontsize=14)\n",
    "    plt.show()\n",
    "    \n",
    "    # 预测图像类别\n",
    "    predict = model(data.unsqueeze(0))\n",
    "    predict = predict[0].cpu().data\n",
    "    probs, topk_index = torch.topk(predict, k=5, dim=-1)\n",
    "    pred_topk = [class_dict[k.item()] for k in topk_index]\n",
    "    print('top-5:')\n",
    "    for pred, prob in zip(pred_topk, probs):\n",
    "        print('{}:  {:.4}%'.format(pred, prob))\n",
    "    print('\\ntop-1:', pred_topk[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
